{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a sample of text chunks extracted from the provided document, formatted as a JSON file for use in a RAG (Retrieval-Augmented Generation) application:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"text_chunks\": [\n",
      "    {\n",
      "      \"chunk_id\": 1,\n",
      "      \"text\": \"Large language models can encode a wealth of semantic knowledge about the world. Such knowledge could be extremely useful to robots aiming to act upon high-level, temporally extended instructions expressed in natural language. However, a significant weakness of language models is that they lack real-world experience, which makes it difficult to leverage them for decision making within a given embodiment.\",\n",
      "      \"source\": \"saycan.pdf\",\n",
      "      \"citation\": \"【4:2†source】\"\n",
      "    },\n",
      "    {\n",
      "      \"chunk_id\": 2,\n",
      "      \"text\": \"The robot can act as the language model’s 'hands and eyes,' while the language model supplies high-level semantic knowledge about the task. We show how low-level skills can be combined with large language models so that the language model provides high-level knowledge about the procedures for performing complex and temporally extended instructions, while value functions associated with these skills provide the grounding necessary to connect this knowledge to a particular physical environment.\",\n",
      "      \"source\": \"saycan.pdf\",\n",
      "      \"citation\": \"【4:2†source】\"\n",
      "    },\n",
      "    {\n",
      "      \"chunk_id\": 3,\n",
      "      \"text\": \"Our method, SayCan, extracts and leverages the knowledge within LLMs in physically-grounded tasks. The LLM (Say) provides a task-grounding to determine useful actions for a high-level goal and the learned affordance functions (Can) provide a world-grounding to determine what is possible to execute upon the plan.\",\n",
      "      \"source\": \"saycan.pdf\",\n",
      "      \"citation\": \"【4:1†source】\"\n",
      "    },\n",
      "    {\n",
      "      \"chunk_id\": 4,\n",
      "      \"text\": \"We use reinforcement learning (RL) as a way to learn language-conditioned value functions that provide affordances of what is possible in the world. We evaluate the proposed approach on 101 real-world robotic tasks that involve a mobile robot accomplishing a large set of language instructions in a real kitchen in a zero-shot fashion.\",\n",
      "      \"source\": \"saycan.pdf\",\n",
      "      \"citation\": \"【4:1†source】\"\n",
      "    },\n",
      "    {\n",
      "      \"chunk_id\": 5,\n",
      "      \"text\": \"SayCan combines probabilities from a LLM (the probability that a skill is useful for the instruction) with the probabilities from a value function (the probability of successfully executing said skill) to select the skill to perform. This emits a skill that is both possible and useful.\",\n",
      "      \"source\": \"saycan.pdf\",\n",
      "      \"citation\": \"【4:5†source】\"\n",
      "    },\n",
      "    {\n",
      "      \"chunk_id\": 6,\n",
      "      \"text\": \"We utilize both BC and RL policy training procedures to obtain the language-conditioned policies and value functions, respectively. For skill specification we use a set of short, natural language descriptions that are represented as language model embeddings.\",\n",
      "      \"source\": \"saycan.pdf\",\n",
      "      \"citation\": \"【4:5†source】\"\n",
      "    },\n",
      "    {\n",
      "      \"chunk_id\": 7,\n",
      "      \"text\": \"To learn language-conditioned BC policies at scale in the real world, we build on top of BC-Z and use a similar policy-network architecture. To learn a language-conditioned RL policy, we use MT-Opt in the Everyday Robots simulator using RetinaGAN sim-to-real transfer.\",\n",
      "      \"source\": \"saycan.pdf\",\n",
      "      \"citation\": \"【4:9†source】\"\n",
      "    },\n",
      "    {\n",
      "      \"chunk_id\": 8,\n",
      "      \"text\": \"SayCan handles multilingual queries out of the box. The results of SayCan on multilingual queries are summarized, and there is almost no performance drop in planning success rate when changing the queries from English to Chinese, French and Spanish.\",\n",
      "      \"source\": \"saycan.pdf\",\n",
      "      \"citation\": \"【4:18†source】\"\n",
      "    },\n",
      "    {\n",
      "      \"chunk_id\": 9,\n",
      "      \"text\": \"For chain-of-thought prompting-based SayCan, we need to modify the prompt to include a part called 'Explanation'. We also slightly change how we use the language model. Instead of directly using the scoring interface to rank possible options, we first use the generative decoding of LLM to create an explanation, and then use the scoring mode, by including the explanation into the prompt.\",\n",
      "      \"source\": \"saycan.pdf\",\n",
      "      \"citation\": \"【4:19†source】\"\n",
      "    },\n",
      "    {\n",
      "      \"chunk_id\": 10,\n",
      "      \"text\": \"Closed-Loop Planning: SayCan only receives environmental feedback through value functions at the current decision step. Owing to the extendibility and the natural language interface, SayCan enables closed-loop planning by leveraging environment feedback through an inner monologue.\",\n",
      "      \"source\": \"saycan.pdf\",\n",
      "      \"citation\": \"【4:19†source】\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "This JSON file contains chunks of text along with their source and citation extracted from the provided document \"saycan.pdf\". Each chunk is uniquely identified by a `chunk_id`. This structure is useful for retrieval-augmented generation systems where relevant text chunks are retrieved based on queries and then used to augment the generation capabilities of a language model.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from openai.types.beta.threads.message_create_params import (\n",
    "    Attachment,\n",
    "    AttachmentToolFileSearch,\n",
    ")\n",
    "\n",
    "filename = \"/home/tyson/echo/resources/example_dataset/robotics/saycan.pdf\"\n",
    "prompt = \"Generate a json file of text chunks I can use for a RAG application\"\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "pdf_assistant = client.beta.assistants.create(\n",
    "    model=\"gpt-4o\",\n",
    "    description=\"An assistant to generate text chunks from a document.\",\n",
    "    tools=[{\"type\": \"file_search\"}],\n",
    "    name=\"PDF assistant\",\n",
    ")\n",
    "\n",
    "# Create thread\n",
    "thread = client.beta.threads.create()\n",
    "file = client.files.create(file=open(filename, \"rb\"), purpose=\"assistants\")\n",
    "\n",
    "# Create assistant\n",
    "client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    attachments=[\n",
    "        Attachment(\n",
    "            file_id=file.id, tools=[AttachmentToolFileSearch(type=\"file_search\")]\n",
    "        )\n",
    "    ],\n",
    "    content=prompt,\n",
    ")\n",
    "\n",
    "# Run thread\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=pdf_assistant.id, timeout=1000\n",
    ")\n",
    "\n",
    "if run.status != \"completed\":\n",
    "    raise Exception(\"Run failed:\", run.status)\n",
    "\n",
    "messages_cursor = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "messages = [message for message in messages_cursor]\n",
    "\n",
    "# Output text\n",
    "res_txt = messages[0].content[0].text.value\n",
    "print(res_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
